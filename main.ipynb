{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tracker\n",
    "\n",
    "This jupyter notebook bundles all necessary high-level steps to train the tracker. We start with running the simulation to generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "sys.path.append('build/')\n",
    "import simulation\n",
    "from time import time, sleep\n",
    "from python.plotter import Plotter\n",
    "from python.GNNtracker import GNNTracker\n",
    "from python.constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get simulation data of the warehouse. We can use the simulation to generate new simulation data. This cell is optional, we can also load old simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulation: 100%|██████████| 60/60 [00:02<00:00, 24.37it/s, Simulated time=0:59 of 1 hours]\n"
     ]
    }
   ],
   "source": [
    "T_simulation = 60 * 60 * 1 # 1 hour\n",
    "\n",
    "sim = simulation.Simulation(\n",
    "    T_step=0.1, \n",
    "    N_humans=3, \n",
    "    N_robots=1\n",
    "    )\n",
    "sim_states = []\n",
    "N_minutes = int(T_simulation / 60)\n",
    "N_hours = int(T_simulation / 3600)\n",
    "pbar = tqdm(range(0, N_minutes), desc='Simulation')\n",
    "for i in pbar:\n",
    "    sim_states += sim.step(int(60 / sim.T_step))\n",
    "    current_hour = int(i / 60)\n",
    "    current_minute = i % 60\n",
    "    pbar.set_postfix({'Simulated time': f'{current_hour}:{current_minute} of {N_hours} hours'})\n",
    "\n",
    "filename = os.path.join(LOG_FOLDER, 'log_' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '.pkl')\n",
    "with open(filename, 'wb') as outp:\n",
    "    pickle.dump({\n",
    "        'T_step': sim.T_step,\n",
    "        'T_simulation': np.arange(0, len(sim_states) * sim.T_step, sim.T_step),\n",
    "        'N_humans': sim.N_humans,\n",
    "        'N_robots': sim.N_robots,\n",
    "        'sim_states': sim_states,\n",
    "    }, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the generated simulation logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"filename\" not in locals():\n",
    "    filename = os.path.join(LOG_FOLDER, 'log_2024-08-16_20-34-51.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    sim_log = pickle.load(f)\n",
    "sim_states = sim_log['sim_states']\n",
    "T_simulation = sim_log['T_simulation']\n",
    "T_step = sim_log['T_step']\n",
    "N_humans = sim_log['N_humans']\n",
    "N_robots = sim_log['N_robots']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the simulation data, we can run the actual tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m node_probabilities \u001b[38;5;241m=\u001b[39m tracker\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot: plotter\u001b[38;5;241m.\u001b[39mupdate(sim_state, node_probabilities)    \n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realtime: sleep(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, T_step \u001b[38;5;241m-\u001b[39m (time() \u001b[38;5;241m-\u001b[39m start)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot = True\n",
    "record_video = False\n",
    "realtime = True\n",
    "\n",
    "tracker = GNNTracker(\n",
    "    T_step=T_step,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "if plot: plotter = Plotter(record_frames=record_video)\n",
    "\n",
    "for sim_state in sim_states:\n",
    "    start = time()\n",
    "    \n",
    "    tracker.add_observation(sim_state)\n",
    "    node_probabilities = tracker.predict()\n",
    "\n",
    "    if plot: plotter.update(sim_state, node_probabilities)    \n",
    "    \n",
    "    if realtime: sleep(max(0, T_step - (time() - start)))\n",
    "\n",
    "if record_video: plotter.create_video(T_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the simulation has been run and the logs have been saved, we can load the loads and extract the data needed for training the Graph Convolutional Net in the tracker. The data is stored in such way that it can be loaded by pytorch-geometric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from python.tracker import Tracker\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data_list = [Data(...), ..., Data(...)]\n",
    "loader = DataLoader(data_list, batch_size=32)\n",
    "\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    sim_log = pickle.load(f)\n",
    "sim_state = sim_log['sim_state']\n",
    "\n",
    "\n",
    "from python.constants import GRAPH_PATH\n",
    "import json\n",
    "\n",
    "# load graph\n",
    "with open(GRAPH_PATH, 'rb') as f:\n",
    "    graph_data = json.load(f)\n",
    "nodes = graph_data['nodes']\n",
    "edges = graph_data['edges']\n",
    "\n",
    "robot_observations = []\n",
    "for i in range(len(sim_state)-1):\n",
    "    probabilities, confidences = Tracker.extract_observation_from_state(sim_state[0])\n",
    "    next_probabilities, next_confidences = Tracker.extract_observation_from_state(sim_state[1])\n",
    "\n",
    "    robot_observations.append(Data(\n",
    "        # x=torch.hstack((torch.tensor(probabilities[:,np.newaxis]), torch.tensor(confidences[:,np.newaxis]))), \n",
    "        x=torch.tensor(probabilities[:,np.newaxis]),\n",
    "        y=torch.tensor(next_probabilities[:,np.newaxis]),\n",
    "        edge_index=torch.tensor(edges).T)\n",
    "        )\n",
    "\n",
    "loader = DataLoader(robot_observations, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a DataLoader for our data, we can define our network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define your layers here\n",
    "        self.conv1 = GCNConv(1, 8)\n",
    "        self.conv2 = GCNConv(8, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "          \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.47653289093650963\n",
      "Epoch 2, Loss: 0.2267185828429523\n",
      "Epoch 3, Loss: 0.16100687461671984\n",
      "Epoch 4, Loss: 0.13177482895166961\n",
      "Epoch 5, Loss: 0.09415652318789017\n",
      "Epoch 6, Loss: 0.06902463443930916\n",
      "Epoch 7, Loss: 0.06070562598813252\n",
      "Epoch 8, Loss: 0.05616342912459074\n",
      "Epoch 9, Loss: 0.05224546466573497\n",
      "Epoch 10, Loss: 0.043209311606710994\n",
      "Epoch 11, Loss: 0.02904659745410954\n",
      "Epoch 12, Loss: 0.022102839624904502\n",
      "Epoch 13, Loss: 0.017662046813551883\n",
      "Epoch 14, Loss: 0.01403605539617939\n",
      "Epoch 15, Loss: 0.012278992077056263\n",
      "Epoch 16, Loss: 0.011746225032124174\n",
      "Epoch 17, Loss: 0.011566076423633437\n",
      "Epoch 18, Loss: 0.011496834899972056\n",
      "Epoch 19, Loss: 0.011461851064601495\n",
      "Epoch 20, Loss: 0.011440883968299814\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is trained and can be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire model saved to gcn_model_full.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'models/gcn_model_full.pth')\n",
    "print('Entire model saved to gcn_model_full.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
